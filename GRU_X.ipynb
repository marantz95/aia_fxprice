{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74278, 7)\n",
      "(30, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./data/training_set.csv')\n",
    "submit = pd.read_csv('./data/sampleSubmission.csv')\n",
    "print(train.shape)\n",
    "print(submit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Time  Weekday     Open     High      Low    Close      Volume\n",
      "0  170 05:00:00        0  1.12053  1.12079  1.12050  1.12067  302.690002\n",
      "1  170 05:10:00        0  1.12066  1.12074  1.12051  1.12070  486.690001\n",
      "2  170 05:20:00        0  1.12070  1.12071  1.12065  1.12070  212.120000\n",
      "3  170 05:30:00        0  1.12070  1.12072  1.12050  1.12061  811.989999\n",
      "4  170 05:40:00        0  1.12060  1.12079  1.12027  1.12029  502.870001\n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'Weekday', 'Open', 'High', 'Low', 'Close', 'Volume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time       0\n",
      "Weekday    0\n",
      "Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Close'] = round(train['Close'], 5)\n",
    "train['Close'] = np.log1p(train['Close'])\n",
    "close = train['Close']\n",
    "\n",
    "target_list = np.expand_dims(close.values,axis=1) # 展開維度成兩維 (總數,feature數)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74278, 1)\n"
     ]
    }
   ],
   "source": [
    "print(target_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = train['Time'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### plt.figure(figsize = (15,7))\n",
    "plt.plot(\n",
    "    np.arange(len(close)),\n",
    "    close,\n",
    "    label = 'Close',\n",
    ")\n",
    "plt.xticks(np.arange(len(close))[::5000], train['Time'][::5000], rotation = 'vertical') # 如果只是看趨勢，也可以不畫X軸資訊，但如果要加建議設個間隔，避免畫面擠滿文數字\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop掉不用的Feature，資料已有時間序列排序，不需要再對時間日期做調整\n",
    "train = train.drop(['Time','Weekday'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74278, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_train (data, target, window_size,predict_length=1):\n",
    "    X_train,y_train = [],[]\n",
    "    for i in range(len(data) - (window_size+predict_length) + 1):\n",
    "        X_train.append(data[i:i+window_size])   \n",
    "        y_train.append(target[i+window_size:i+window_size+predict_length])\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    return X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "X,y = window_train(train, target_list, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74263, 15, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # (總數,window_size,feature數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74263, 1, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74263, 15)\n",
      "(74263, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "# 0:Open,1:High,2:Low,3:Close,4:Volume\n",
    "X = X[:,:,3]          # 只取Close (第三個column)\n",
    "print(X.shape)\n",
    "X = X[:,:,np.newaxis] # 展維度\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (74263, 15, 1)\n",
      "y_train size: (74263, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# 前七萬筆當作訓練資料\n",
    "# 不切training set, train完後產生預測資料\n",
    "X_train = X[0:]\n",
    "y_train = y[0:]\n",
    "\n",
    "# 七萬筆後當驗證資料\n",
    "#X_valid = X[70000:]\n",
    "#y_valid = y[70000:]\n",
    "\n",
    "print(\"X_train size: {}\".format(X_train.shape))\n",
    "print(\"y_train size: {}\".format(y_train.shape))\n",
    "#print(\"X_test size: {}\".format(X_valid.shape))\n",
    "#print(\"y_test size: {}\".format(y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立、訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, LSTM, SimpleRNN, GRU\n",
    "from tensorflow.python.keras.layers import TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OK Case\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=32, activation='tanh', return_sequences=True))\n",
    "#model.add(LSTM(units=16, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=16, activation='tanh', return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=16))\n",
    "model.add(Dense(units=8))\n",
    "#model.add(Dense(units=8))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "opt = Adam(lr=0.001, decay=1e-6, clipvalue=5)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Case 2 RELU 0.00634 Not good....\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=32, activation='relu', return_sequences=True))\n",
    "# model.add(LSTM(units=16, activation='relu', return_sequences=True))\n",
    "# model.add(LSTM(units=16, activation='relu', return_sequences=False))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=16))\n",
    "# model.add(Dense(units=8))\n",
    "# #model.add(Dense(units=8))\n",
    "# model.add(Dense(units=1))\n",
    "\n",
    "# opt = Adam(lr=0.001, decay=1e-6, clipvalue=5)\n",
    "# model.compile(loss='mse', optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Case3 sigmoid Not good.....\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=32, activation='sigmoid', return_sequences=True))\n",
    "# #model.add(LSTM(units=16, activation='tanh', return_sequences=True))\n",
    "# model.add(LSTM(units=16, activation='sigmoid', return_sequences=False))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=16))\n",
    "# model.add(Dense(units=8))\n",
    "# #model.add(Dense(units=8))\n",
    "# model.add(Dense(units=1))\n",
    "\n",
    "# opt = Adam(lr=0.001, decay=1e-6, clipvalue=5)\n",
    "# model.compile(loss='mse', optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## tanh with SGD Not good....\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=32, activation='tanh', return_sequences=True))\n",
    "# #model.add(LSTM(units=16, activation='tanh', return_sequences=True))\n",
    "# model.add(LSTM(units=16, activation='tanh', return_sequences=False))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=16))\n",
    "# model.add(Dense(units=8))\n",
    "# #model.add(Dense(units=8))\n",
    "# model.add(Dense(units=1))\n",
    "\n",
    "# opt = SGD(lr=0.001, decay=1e-6, clipvalue=5)\n",
    "# model.compile(loss='mse', optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OK Case 0.00074 Very Good !\n",
    "model = Sequential()\n",
    "model.add(GRU(units=32, activation='tanh', return_sequences=True))\n",
    "#model.add(LSTM(units=16, activation='tanh', return_sequences=True))\n",
    "model.add(GRU(units=16, activation='tanh', return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=16))\n",
    "model.add(Dense(units=8))\n",
    "#model.add(Dense(units=8))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "opt = Adam(lr=0.001, decay=1e-6, clipvalue=5)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74263 samples\n",
      "Epoch 1/2\n",
      "74263/74263 [==============================] - 110s 1ms/sample - loss: 0.0010 - mse: 0.0010\n",
      "Epoch 2/2\n",
      "74263/74263 [==============================] - 105s 1ms/sample - loss: 2.6147e-06 - mse: 2.6147e-06\n"
     ]
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, epochs=15, validation_data=(X_valid,y_valid))\n",
    "model.fit(X_train, y_train, epochs=2, )\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    multiple                  3264      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  multiple                  2352      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             multiple                  272       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             multiple                  136       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  9         \n",
      "=================================================================\n",
      "Total params: 6,033\n",
      "Trainable params: 6,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估模型準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-eb492a92efa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_valid' is not defined"
     ]
    }
   ],
   "source": [
    "valid_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-96a3580e3c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Original'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_pred' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(np.squeeze(valid_pred),label='pred')\n",
    "plt.plot(np.squeeze(y_valid),label='Original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "[0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089]\n"
     ]
    }
   ],
   "source": [
    "# 取X中最後一組資料筆數\n",
    "test_data = np.array([i[0] for i in X[-1]])\n",
    "print(test_data.shape)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n",
      "[0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452]\n"
     ]
    }
   ],
   "source": [
    "# 將資料最後一筆做串接用於預測下一筆資料\n",
    "test_data = np.append(test_data,y[-1])\n",
    "print(test_data.shape)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.75511678 0.75513088 0.75516377 0.75493348 0.75457148 0.75464671\n",
      " 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186 0.75469843\n",
      " 0.75459969 0.75458089 0.75444452]\n",
      "pred:  [[0.7543746]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462]\n",
      "1\n",
      "[0.75513088 0.75516377 0.75493348 0.75457148 0.75464671 0.75475015\n",
      " 0.75475955 0.75473134 0.75454797 0.75480186 0.75469843 0.75459969\n",
      " 0.75458089 0.75444452 0.75437462]\n",
      "pred:  [[0.7543213]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128]\n",
      "2\n",
      "[0.75516377 0.75493348 0.75457148 0.75464671 0.75475015 0.75475955\n",
      " 0.75473134 0.75454797 0.75480186 0.75469843 0.75459969 0.75458089\n",
      " 0.75444452 0.75437462 0.75432128]\n",
      "pred:  [[0.7542625]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251]\n",
      "3\n",
      "[0.75493348 0.75457148 0.75464671 0.75475015 0.75475955 0.75473134\n",
      " 0.75454797 0.75480186 0.75469843 0.75459969 0.75458089 0.75444452\n",
      " 0.75437462 0.75432128 0.75426251]\n",
      "pred:  [[0.7542012]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117]\n",
      "4\n",
      "[0.75457148 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797\n",
      " 0.75480186 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462\n",
      " 0.75432128 0.75426251 0.75420117]\n",
      "pred:  [[0.7541389]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889]\n",
      "5\n",
      "[0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889]\n",
      "pred:  [[0.75407624]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624]\n",
      "6\n",
      "[0.75475015 0.75475955 0.75473134 0.75454797 0.75480186 0.75469843\n",
      " 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128 0.75426251\n",
      " 0.75420117 0.75413889 0.75407624]\n",
      "pred:  [[0.7540134]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342]\n",
      "7\n",
      "[0.75475955 0.75473134 0.75454797 0.75480186 0.75469843 0.75459969\n",
      " 0.75458089 0.75444452 0.75437462 0.75432128 0.75426251 0.75420117\n",
      " 0.75413889 0.75407624 0.75401342]\n",
      "pred:  [[0.7539505]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048]\n",
      "8\n",
      "[0.75473134 0.75454797 0.75480186 0.75469843 0.75459969 0.75458089\n",
      " 0.75444452 0.75437462 0.75432128 0.75426251 0.75420117 0.75413889\n",
      " 0.75407624 0.75401342 0.75395048]\n",
      "pred:  [[0.7538877]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771]\n",
      "9\n",
      "[0.75454797 0.75480186 0.75469843 0.75459969 0.75458089 0.75444452\n",
      " 0.75437462 0.75432128 0.75426251 0.75420117 0.75413889 0.75407624\n",
      " 0.75401342 0.75395048 0.75388771]\n",
      "pred:  [[0.75382495]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495]\n",
      "10\n",
      "[0.75480186 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462\n",
      " 0.75432128 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342\n",
      " 0.75395048 0.75388771 0.75382495]\n",
      "pred:  [[0.7537623]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623 ]\n",
      "11\n",
      "[0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623 ]\n",
      "pred:  [[0.75369954]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954]\n",
      "12\n",
      "[0.75459969 0.75458089 0.75444452 0.75437462 0.75432128 0.75426251\n",
      " 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048 0.75388771\n",
      " 0.75382495 0.7537623  0.75369954]\n",
      "pred:  [[0.75363696]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696]\n",
      "13\n",
      "[0.75458089 0.75444452 0.75437462 0.75432128 0.75426251 0.75420117\n",
      " 0.75413889 0.75407624 0.75401342 0.75395048 0.75388771 0.75382495\n",
      " 0.7537623  0.75369954 0.75363696]\n",
      "pred:  [[0.7535745]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449]\n",
      "14\n",
      "[0.75444452 0.75437462 0.75432128 0.75426251 0.75420117 0.75413889\n",
      " 0.75407624 0.75401342 0.75395048 0.75388771 0.75382495 0.7537623\n",
      " 0.75369954 0.75363696 0.75357449]\n",
      "pred:  [[0.7535119]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191]\n",
      "15\n",
      "[0.75437462 0.75432128 0.75426251 0.75420117 0.75413889 0.75407624\n",
      " 0.75401342 0.75395048 0.75388771 0.75382495 0.7537623  0.75369954\n",
      " 0.75363696 0.75357449 0.75351191]\n",
      "pred:  [[0.7534494]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938]\n",
      "16\n",
      "[0.75432128 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342\n",
      " 0.75395048 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696\n",
      " 0.75357449 0.75351191 0.75344938]\n",
      "pred:  [[0.753387]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697]\n",
      "17\n",
      "[0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697]\n",
      "pred:  [[0.75332457]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457]\n",
      "18\n",
      "[0.75420117 0.75413889 0.75407624 0.75401342 0.75395048 0.75388771\n",
      " 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449 0.75351191\n",
      " 0.75344938 0.75338697 0.75332457]\n",
      "pred:  [[0.7532622]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222]\n",
      "19\n",
      "[0.75413889 0.75407624 0.75401342 0.75395048 0.75388771 0.75382495\n",
      " 0.7537623  0.75369954 0.75363696 0.75357449 0.75351191 0.75344938\n",
      " 0.75338697 0.75332457 0.75326222]\n",
      "pred:  [[0.7531998]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982]\n",
      "20\n",
      "[0.75407624 0.75401342 0.75395048 0.75388771 0.75382495 0.7537623\n",
      " 0.75369954 0.75363696 0.75357449 0.75351191 0.75344938 0.75338697\n",
      " 0.75332457 0.75326222 0.75319982]\n",
      "pred:  [[0.7531376]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759]\n",
      "21\n",
      "[0.75401342 0.75395048 0.75388771 0.75382495 0.7537623  0.75369954\n",
      " 0.75363696 0.75357449 0.75351191 0.75344938 0.75338697 0.75332457\n",
      " 0.75326222 0.75319982 0.75313759]\n",
      "pred:  [[0.7530753]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753 ]\n",
      "22\n",
      "[0.75395048 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696\n",
      " 0.75357449 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222\n",
      " 0.75319982 0.75313759 0.7530753 ]\n",
      "pred:  [[0.753013]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301]\n",
      "23\n",
      "[0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301]\n",
      "pred:  [[0.75295085]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085]\n",
      "24\n",
      "[0.75382495 0.7537623  0.75369954 0.75363696 0.75357449 0.75351191\n",
      " 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982 0.75313759\n",
      " 0.7530753  0.75301301 0.75295085]\n",
      "pred:  [[0.7528887]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868]\n",
      "25\n",
      "[0.7537623  0.75369954 0.75363696 0.75357449 0.75351191 0.75344938\n",
      " 0.75338697 0.75332457 0.75326222 0.75319982 0.75313759 0.7530753\n",
      " 0.75301301 0.75295085 0.75288868]\n",
      "pred:  [[0.75282663]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868 0.75282663]\n",
      "26\n",
      "[0.75369954 0.75363696 0.75357449 0.75351191 0.75344938 0.75338697\n",
      " 0.75332457 0.75326222 0.75319982 0.75313759 0.7530753  0.75301301\n",
      " 0.75295085 0.75288868 0.75282663]\n",
      "pred:  [[0.75276446]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868 0.75282663\n",
      " 0.75276446]\n",
      "27\n",
      "[0.75363696 0.75357449 0.75351191 0.75344938 0.75338697 0.75332457\n",
      " 0.75326222 0.75319982 0.75313759 0.7530753  0.75301301 0.75295085\n",
      " 0.75288868 0.75282663 0.75276446]\n",
      "pred:  [[0.7527025]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868 0.75282663\n",
      " 0.75276446 0.75270247]\n",
      "28\n",
      "[0.75357449 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222\n",
      " 0.75319982 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868\n",
      " 0.75282663 0.75276446 0.75270247]\n",
      "pred:  [[0.7526404]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868 0.75282663\n",
      " 0.75276446 0.75270247 0.75264043]\n",
      "29\n",
      "[0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868 0.75282663\n",
      " 0.75276446 0.75270247 0.75264043]\n",
      "pred:  [[0.7525783]]\n",
      "new data:  [0.75522956 0.75511678 0.75513088 0.75516377 0.75493348 0.75457148\n",
      " 0.75464671 0.75475015 0.75475955 0.75473134 0.75454797 0.75480186\n",
      " 0.75469843 0.75459969 0.75458089 0.75444452 0.75437462 0.75432128\n",
      " 0.75426251 0.75420117 0.75413889 0.75407624 0.75401342 0.75395048\n",
      " 0.75388771 0.75382495 0.7537623  0.75369954 0.75363696 0.75357449\n",
      " 0.75351191 0.75344938 0.75338697 0.75332457 0.75326222 0.75319982\n",
      " 0.75313759 0.7530753  0.75301301 0.75295085 0.75288868 0.75282663\n",
      " 0.75276446 0.75270247 0.75264043 0.75257832]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(submit)):\n",
    "    print(i)\n",
    "    test_X_data = test_data[-len(test_data)+1+i:]\n",
    "    print(test_X_data)\n",
    "    \n",
    "    # 將其reshape成三維shape\n",
    "    test_X_data = test_X_data[np.newaxis,:,np.newaxis]\n",
    "    \n",
    "    # 用最後window_size筆預測下一筆\n",
    "    test_pred = model.predict(test_X_data)\n",
    "    print('pred: ',test_pred)\n",
    "\n",
    "    # append 新預測至最後一筆\n",
    "    test_data = np.append(test_data, test_pred)\n",
    "    print('new data: ',test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Close'] = test_data[window_size+1:]\n",
    "submit['Close'] = np.expm1(submit['Close'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>895 19:00:00</td>\n",
       "      <td>1.126281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>895 19:10:00</td>\n",
       "      <td>1.126168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>895 19:20:00</td>\n",
       "      <td>1.126043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895 19:30:00</td>\n",
       "      <td>1.125913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>895 19:40:00</td>\n",
       "      <td>1.125780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>895 19:50:00</td>\n",
       "      <td>1.125647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>895 20:00:00</td>\n",
       "      <td>1.125513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>895 20:10:00</td>\n",
       "      <td>1.125380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>895 20:20:00</td>\n",
       "      <td>1.125246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>895 20:30:00</td>\n",
       "      <td>1.125113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>895 20:40:00</td>\n",
       "      <td>1.124980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>895 20:50:00</td>\n",
       "      <td>1.124846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>895 21:00:00</td>\n",
       "      <td>1.124713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>895 21:10:00</td>\n",
       "      <td>1.124581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>895 21:20:00</td>\n",
       "      <td>1.124448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>895 21:30:00</td>\n",
       "      <td>1.124315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>895 21:40:00</td>\n",
       "      <td>1.124182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>895 21:50:00</td>\n",
       "      <td>1.124050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>895 22:00:00</td>\n",
       "      <td>1.123917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>895 22:10:00</td>\n",
       "      <td>1.123785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>895 22:20:00</td>\n",
       "      <td>1.123653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>895 22:30:00</td>\n",
       "      <td>1.123520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>895 22:40:00</td>\n",
       "      <td>1.123388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>895 22:50:00</td>\n",
       "      <td>1.123256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>895 23:00:00</td>\n",
       "      <td>1.123124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>895 23:10:00</td>\n",
       "      <td>1.122992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>895 23:20:00</td>\n",
       "      <td>1.122860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>895 23:30:00</td>\n",
       "      <td>1.122729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>895 23:40:00</td>\n",
       "      <td>1.122597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>895 23:50:00</td>\n",
       "      <td>1.122465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time     Close\n",
       "0   895 19:00:00  1.126281\n",
       "1   895 19:10:00  1.126168\n",
       "2   895 19:20:00  1.126043\n",
       "3   895 19:30:00  1.125913\n",
       "4   895 19:40:00  1.125780\n",
       "5   895 19:50:00  1.125647\n",
       "6   895 20:00:00  1.125513\n",
       "7   895 20:10:00  1.125380\n",
       "8   895 20:20:00  1.125246\n",
       "9   895 20:30:00  1.125113\n",
       "10  895 20:40:00  1.124980\n",
       "11  895 20:50:00  1.124846\n",
       "12  895 21:00:00  1.124713\n",
       "13  895 21:10:00  1.124581\n",
       "14  895 21:20:00  1.124448\n",
       "15  895 21:30:00  1.124315\n",
       "16  895 21:40:00  1.124182\n",
       "17  895 21:50:00  1.124050\n",
       "18  895 22:00:00  1.123917\n",
       "19  895 22:10:00  1.123785\n",
       "20  895 22:20:00  1.123653\n",
       "21  895 22:30:00  1.123520\n",
       "22  895 22:40:00  1.123388\n",
       "23  895 22:50:00  1.123256\n",
       "24  895 23:00:00  1.123124\n",
       "25  895 23:10:00  1.122992\n",
       "26  895 23:20:00  1.122860\n",
       "27  895 23:30:00  1.122729\n",
       "28  895 23:40:00  1.122597\n",
       "29  895 23:50:00  1.122465"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./data/submit.xxx.csv',index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
